from __future__ import annotations

from typing import Any, Dict, List

from src.common.datetime import now_wib
from src.common.log import get_logger
from src.common import sb as sbapi
from src.workflow.config import ALL_TAGS


from .models import Workflow, WorkflowEvent
from .rules import build_events_for_row

logger = get_logger("workflow.engine")


def _normalize_list(val: Any) -> List[str]:
    if val is None:
        return []
    if isinstance(val, list):
        return [str(v).strip() for v in val if str(v).strip()]
    if isinstance(val, str):
        s = val.strip()
        if not s:
            return []
        try:
            import json
            if s.startswith("[") or s.startswith("{"):
                parsed = json.loads(s.replace("{", "[").replace("}", "]"))
                if isinstance(parsed, list):
                    return [str(v).strip() for v in parsed if str(v).strip()]
        except Exception:
            pass
        return [p.strip() for p in s.split(",") if p.strip()]
    return [str(val).strip()]


async def fetch_active_workflows() -> List[Workflow]:
    rows = await sbapi.fetch_all(
        table="user_workflow",
        select="id,user_id,name,tickers,tags,channels,is_active,sectors,sub_sectors,industries,sub_industries",
        filters=[("is_active", "eq.true")],
    )

    workflows: List[Workflow] = []
    for r in rows:
        raw_channels = r.get("channels") or {}
        sectors = _normalize_list(r.get("sectors"))
        sub_sectors = _normalize_list(r.get("sub_sectors"))
        industries = _normalize_list(r.get("industries"))
        sub_industries = _normalize_list(r.get("sub_industries"))

        channels: Dict[str, Any]

        if isinstance(raw_channels, dict):
            channels = raw_channels

        elif isinstance(raw_channels, list):
            tmp: Dict[str, Any] = {}
            for item in raw_channels:
                if not isinstance(item, dict):
                    continue
                # each item should be a single-key dict
                for key, value in item.items():
                    if isinstance(value, dict):
                        tmp[key] = value
                    else:
                        tmp[key] = {"value": value}
            channels = tmp

        else:
            # Unknown structure; fall back to empty
            channels = {}

        workflows.append(
            Workflow(
                id=str(r["id"]),
                user_id=int(r["user_id"]),
                name=r.get("name") or "",
                tickers=_normalize_list(r.get("tickers")),
                tags=_normalize_list(r.get("tags")),
                sectors=sectors,
                sub_sectors=sub_sectors,
                industries=industries,
                sub_industries=sub_industries,
                channels=channels,
                is_active=bool(r.get("is_active")),
            )
        )

    logger.info("Found %d active workflows", len(workflows))
    return workflows


async def fetch_mv_for_tickers(tickers: List[str]) -> List[Dict[str, Any]]:
    """
    Fetch MV rows for the given tickers.
    If tickers is empty, treat it as wildcard (fetch all rows).
    """
    if not tickers:
        return await sbapi.fetch_all(
            table="idx_workflow_data",
            select="*",
        )
    return await sbapi.fetch_all(
        table="idx_workflow_data",
        select="*",
        in_filters={"symbol": tickers},
    )


async def generate_events() -> List[WorkflowEvent]:
    """
    Main engine: baca user_workflow + idx_workflow_data â†’ list WorkflowEvent.
    (Belum mengirim ke channel; itu dihandle di runner.py + channels/*)
    """
    now = now_wib()
    logger.info("Running sectors workflow at %s", now.isoformat())

    workflows = await fetch_active_workflows()
    logger.info("Found %d active workflows", len(workflows))

    all_events: List[WorkflowEvent] = []

    # DEBUG: print all events generated by the engine
    from src.common.log import get_logger
    dbg = get_logger("workflow.debug")
    for ev in all_events:
        dbg.info("Event generated: workflow_id=%s tag=%s symbol=%s", ev.workflow_id, ev.tag, ev.symbol)

    for wf in workflows:
        if not wf.is_active:
            continue

        # Wildcard semantics:
        # - tickers=[] -> all symbols
        # - tags=[]    -> all tags (ALL_TAGS)
        rows = await fetch_mv_for_tickers(wf.tickers)
        logger.info("Workflow %s (%s): %d MV rows", wf.id, wf.name, len(rows))

        for row in rows:
            if not wf.tickers:
                def _match(field: str, allowed: List[str]) -> bool:
                    if not allowed:
                        return True
                    val = str(row.get(field) or "").strip().lower()
                    return val in allowed

                if not all([
                    _match("sector", wf.sectors),
                    _match("sub_sector", wf.sub_sectors),
                    _match("industry", wf.industries),
                    _match("sub_industry", wf.sub_industries),
                ]):
                    continue

            # Clone tags per call to avoid mutating the dataclass
            wf_for_row = Workflow(
                id=wf.id,
                user_id=wf.user_id,
                name=wf.name,
                tickers=wf.tickers,
                tags=wf.tags or list(ALL_TAGS),
                sectors=wf.sectors,
                sub_sectors=wf.sub_sectors,
                industries=wf.industries,
                sub_industries=wf.sub_industries,
                channels=wf.channels,
                is_active=wf.is_active,
            )
            events = build_events_for_row(wf_for_row, row, now=now)
            all_events.extend(events)

    logger.info("Generated %d workflow events", len(all_events))
    return all_events
