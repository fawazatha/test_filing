name: IDX Filings

on:
  schedule:
    - cron: "0 */2 * * 1-5"
  workflow_dispatch:
    inputs:
      mode:
        description: "Run mode"
        type: choice
        required: true
        default: lookback
        options: [single_day, range, lookback]
      time:
        description: |
          For single_day: YYYYMMDD|HH:MM|HH:MM
          For range     : YYYYMMDD..YYYYMMDD
          For lookback  : minutes (integer). Example: 2 hours = 120
        required: false
      refresh_company_assets:
        description: "Refresh company map + latest prices before run? (handled in a separate warm-up step)"
        type: boolean
        default: true
      prices_lookback_days:
        description: "Lookback days for latest prices"
        required: false
        default: "14"
      no_price_fallback:
        description: "Disable per-symbol fallback (not recommended)"
        type: boolean
        default: false
      upload_to_supabase:
        description: "Upload filings to Supabase? (needs secrets)"
        type: boolean
        default: false
      upload_filings_table:
        description: "Supabase table for filings upload (with --upload)"
        required: false
        default: "idx_filings"
      send_alerts:
        description: "Send SES email alerts?"
        type: boolean
        default: false
      upload_news:
        description: "Upload generated articles to Supabase (idx_news or SUPABASE_NEWS_TABLE)?"
        type: boolean
        default: true
      verbose:
        description: "Verbose logs?"
        type: boolean
        default: true

env:
  PYTHONPATH: src
  TZ: Asia/Jakarta
  OVERLAP_MINUTES: "1"
  COMPANY_MAP_SCRIPT: src/scripts/company_map_hybrid.py
  LATEST_PRICES_SCRIPT: ""
  # SUPABASE_NEWS_TABLE: idx_news

concurrency:
  group: idx-filings-manual
  cancel-in-progress: true

jobs:
  run:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    steps:
      - name: Checkout
        uses: actions/checkout@v4

      - name: Setup Python
        uses: actions/setup-python@v5
        with:
          python-version: "3.10"
          cache: "pip"

      - name: Install dependencies
        run: |
          python -m pip install -U pip
          pip install -r requirements.txt

      - name: Pre-clean
        run: |
          rm -rf downloads/idx-format downloads/non-idx-format || true
          rm -rf data/*.json data/*.jsonl alerts/*.json alerts_inserted alerts_not_inserted artifacts/*.zip || true
          mkdir -p downloads/idx-format downloads/non-idx-format data alerts artifacts

      - name: Compute run timestamp (WIB)
        id: ts
        run: echo "stamp=$(date +'%Y%m%d-%H%M%S')" >> "$GITHUB_OUTPUT"

      # Company assets warm-up (single-source)
      - name: Company map (single-source) â€” refresh always
        if: ${{ (github.event_name == 'schedule') || (inputs.refresh_company_assets == true) }}
        env:
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}
          COMPANY_MAP_FORCE_REFRESH: "1"
        run: |
          set -euo pipefail

          # Ensure minimal deps if the single-source script needs them
          python - <<'PY'
          import importlib, sys
          for m in ("requests","dotenv"):
              try: importlib.import_module(m)
              except Exception: sys.exit(1)
          PY
          if [[ $? -ne 0 ]]; then
            pip install requests python-dotenv
          fi

          echo "[Warm-up] Refresh company_map via ${COMPANY_MAP_SCRIPT}"
          python "${COMPANY_MAP_SCRIPT}" refresh

          echo "[Warm-up] Status company_map:"
          python "${COMPANY_MAP_SCRIPT}" status || true

      # Main pipeline
      - name: Run orchestrator
        env:
          # App secrets
          SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
          SUPABASE_KEY: ${{ secrets.SUPABASE_KEY }}

          # (Optional) HTTP proxy only for IDX fetch/downloader
          HTTP_PROXY:  ${{ secrets.PROXY }}
          HTTPS_PROXY: ${{ secrets.PROXY }}
          NO_PROXY: "localhost,127.0.0.1,supabase.co,.supabase.co,amazonaws.com,.amazonaws.com"

          # AWS / SES
          AWS_ACCESS_KEY_ID:     ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION:            ${{ secrets.AWS_REGION }}
          SES_FROM_EMAIL:        ${{ secrets.SES_FROM_EMAIL }}
          ALERT_TO_EMAIL_INSERTED:     ${{ secrets.ALERT_TO_EMAIL }}
          ALERT_TO_EMAIL_NOT_INSERTED: ${{ secrets.ALERT_TO_EMAIL }}

          # (Optional) LLM keys
          GROQ_API_KEY:   ${{ secrets.GROQ_API_KEY }}
          OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
          GEMINI_API_KEY: ${{ secrets.GEMINI_API_KEY }}
          SUPABASE_NEWS_TABLE:   ${{ secrets.SUPABASE_NEWS_TABLE }}
        shell: bash
        run: |
          set -euo pipefail

          add_arg_if_set() {
            local flag="$1"; shift
            local val="${1:-}"
            if [[ -n "${val}" ]]; then ARGS+=("$flag" "$val"); fi
          }

          ARGS=( "--zip-artifacts" "--artifact-prefix" "filings"
                 "--company-map-script" "${COMPANY_MAP_SCRIPT}" )

          # Pass latest prices script (for logging/status only)
          if [[ -n "${LATEST_PRICES_SCRIPT}" ]]; then
            ARGS+=( "--latest-prices-script" "${LATEST_PRICES_SCRIPT}" )
          fi

          # Verbose?
          if [[ "${{ inputs.verbose }}" == "true" || "${{ github.event_name }}" == "schedule" ]]; then
            ARGS+=("-v")
          fi

          MODE="${{ inputs.mode }}"
          TIME_RAW="${{ inputs.time }}"
          TIME="$(echo "${TIME_RAW:-}" | tr -d '[:space:]')"
          digits_only() { printf '%s' "$1" | tr -cd '0-9'; }

          DEFAULT_WINDOW_MINUTES=$(( 120 + ${OVERLAP_MINUTES:-1} ))
          DEFAULT_PRICES_LOOKBACK_DAYS="14"

          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            PRICES_LOOKBACK_DAYS="${DEFAULT_PRICES_LOOKBACK_DAYS}"
          else
            PRICES_LOOKBACK_DAYS="${{ inputs.prices_lookback_days }}"
            [[ -z "${PRICES_LOOKBACK_DAYS:-}" ]] && PRICES_LOOKBACK_DAYS="${DEFAULT_PRICES_LOOKBACK_DAYS}"
          fi

          # Mode window / date
          if [[ "${{ github.event_name }}" == "schedule" ]]; then
            # Cron: keep default window; upload to default table
            ARGS+=( "--window-minutes" "${DEFAULT_WINDOW_MINUTES}" "--upload" "--table" "idx_filings" )
            add_arg_if_set "--prices-lookback-days" "${PRICES_LOOKBACK_DAYS}"
            [[ "${{ inputs.no_price_fallback }}" == "true" ]] && ARGS+=( "--no-price-fallback" )
          else
            case "${MODE}" in
              single_day)
                if [[ -z "${TIME:-}" || "${TIME}" != *"|"* ]]; then
                  echo "::error::For mode=single_day, 'time' must be 'YYYYMMDD|HH:MM|HH:MM'"; exit 1;
                fi
                IFS='|' read -r DATE SD ED <<< "$TIME"
                DATE="$(digits_only "${DATE}")"
                SD="${SD//[^0-9:]/}"
                ED="${ED//[^0-9:]/}"
                [[ ${#DATE} -eq 8 ]] || { echo "::error::DATE must be YYYYMMDD"; exit 1; }
                [[ "$SD" =~ ^[0-2][0-9]:[0-5][0-9]$ ]] || { echo "::error::start time must be HH:MM"; exit 1; }
                [[ "$ED" =~ ^[0-2][0-9]:[0-5][0-9]$ ]] || { echo "::error::end time must be HH:MM"; exit 1; }
                ARGS+=( "--date" "$DATE" "--start-hhmm" "$SD" "--end-hhmm" "$ED" )
                ;;
              range)
                if [[ -z "${TIME:-}" || "${TIME}" != *".."* ]]; then
                  echo "::error::For mode=range, 'time' must be 'YYYYMMDD..YYYYMMDD'"; exit 1;
                fi
                IFS='..' read -r FROMD TOD <<< "$TIME"
                FROMD="$(digits_only "${FROMD}")"
                TOD="$(digits_only "${TOD}")"
                [[ ${#FROMD} -eq 8 && ${#TOD} -eq 8 ]] || { echo "::error::from/to must be YYYYMMDD"; exit 1; }
                ARGS+=( "--from-date" "$FROMD" "--to-date" "$TOD" )
                ;;
              lookback|*)
                # LOOKBACK: minutes only (integer)
                if [[ -z "${TIME:-}" ]]; then
                  WIN="${DEFAULT_WINDOW_MINUTES}"
                else
                  if [[ ! "${TIME}" =~ ^[0-9]+$ ]]; then
                    echo "::error::For mode=lookback, 'time' must be an integer (minutes), e.g., 120"; exit 1;
                  fi
                  WIN="${TIME}"
                fi
                ARGS+=( "--window-minutes" "$WIN" )
                ;;
            esac

            if [[ "${{ inputs.upload_to_supabase }}" == "true" ]]; then
              TABLE="${{ inputs.upload_filings_table }}"
              [[ -z "${TABLE:-}" ]] && TABLE="idx_filings"
              ARGS+=( "--upload" "--table" "$TABLE" )
            fi

            add_arg_if_set "--prices-lookback-days" "${PRICES_LOOKBACK_DAYS}"
            [[ "${{ inputs.no_price_fallback }}" == "true" ]] && ARGS+=( "--no-price-fallback" )
          fi

          # Articles generation
          ARGS+=( "--generate-articles" "--articles-out" "data/articles.jsonl" )

          # Optional upload news
          if [[ "${{ inputs.upload_news }}" == "true" || "${{ github.event_name }}" == "schedule" ]]; then
            ARGS+=( "--upload-news" "--prefer-symbol" "--news-table" "idx_news" )
          fi

          # LLM toggling
          if [[ -n "${GROQ_API_KEY:-}" || -n "${OPENAI_API_KEY:-}" || -n "${GEMINI_API_KEY:-}" ]]; then
            ARGS+=( "--use-llm" )
          else
            echo "::notice::No LLM API key found; continuing without LLM."
          fi

          # Email alerts toggling
          if [[ "${{ github.event_name }}" == "schedule" || "${{ inputs.send_alerts }}" == "true" ]]; then
            ARGS+=( "--email-alerts" )
          else
            echo "::notice::Email alerts disabled for this run."
          fi

          echo "python -m pipeline.orchestrator ${ARGS[*]}"
          python -m pipeline.orchestrator "${ARGS[@]}"

      - name: Upload artifacts
        uses: actions/upload-artifact@v4
        with:
          name: filings-manual-${{ steps.ts.outputs.stamp }}-run${{ github.run_number }}
          path: |
            artifacts/*.zip
            data/*.json
            data/*.jsonl
            data/company/*.json
            alerts/*.json
            alerts_inserted/*.json
            alerts_not_inserted/*.json
          if-no-files-found: warn
          retention-days: 14
          compression-level: 6
